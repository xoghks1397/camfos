지도학습

k-최근접 이웃 알고리즘(분류와 회귀에 사용가능)

단순히 훈련 데이터를 저장하여 만든 후 새 데이터 포인트에서 가장 가까운 훈련 데이트 포인트를 찾아 훈련데이터를 새데잍 포인터로 지정
k개의 이웃을 찾는다는 의미
이웃을 많이 사용하면 복잡도가 낮아지고 단순해지면서 안정된 예측 가능 조금 사용하면 복잡도가 높아지고 안정된 예측 안됨.
매우 이해하기 쉬운 모델 but 훈련세트가 크면 예측이 어려움.
예측이 느리고 많은 특성을 처리하는능력이 부족

회귀의 선형모델

특성이 하나일떈, 직선, 두개 평면, 많은 특성은 초평면
그래서 특성이 적으면 데이터의 상세정보를 잃는다. but 특성이 많은 데이터 셋이면 높은 성능

선형 회귀(최소 제곱법)
간당하고 오래된 회귀영 선형 알고리즘
예측과 훈련 세트에 있는 평균제곱오차를 최소화하는 w와 b를찾는다.
w(기울기 파라미터) : 가중치,계수 -> coef_속성에 저장 
b(편향 절편 파라미터) : intercept_속성에 저장
저차원에선 과대적합을 걱정x, but 고차원 데이터 셋에선 과대적합이 될 가능성 높음
  
리지 회귀
기본 선형 회귀 모델 대신 사용하는 알고리즘(리지도 회귀를 위한 선형모델 이긴하다.)
과대적합되어 복잡도를 제어할 수 있게 설정된 모델(가중치(W) :제약 조건 걸기)
복잡도가 낮아질수록 일반화(정규화)한 모델이 된다.
alpha값으로 조절 가능 (기본값 1.0) 알파값 올라가면 계수는 0으로 수렴 (L2규제)
높은 alpha값은 제약이 더많은 모델이므로 작은 alpha값 보다 coef_(절편)의 절댓값 크기는 작다.

라쏘(lasso)
선형 회귀에 규제를 적용하는데 Ridge(리지)의 대안
계수를 0으로 만든다.(L1규제) -> 제외되는 특성 존재 ->모델이해가 쉬워지고 모델의 가장 중요한 특성이 무엇인지 암(분석 용이)
alpha값을 낮추면 모델 복잡도 증가 ->훈련,테스트 성능 증가(but, 너무 낮추면 과대 적합됨)
보통 리지 회귀를 선호하지만, 분석을 용이하게하고 싶다면 라쏘 회귀를 사용


분류형 선형 모델

이진 분류
선형 회귀와 아주 비슷.but 가중치(w)합을 그냥사용(선형 회귀)  이진 분류는 예측한 값을 임계치 0 과 비교합니다.
회귀용 선형 모델은 직선 평면 초평면 출력이 y이 특성의 선형함수 but, 결정 경계가 입력의 선형 함수 (두개의 클래스로 구분하는 분류기)
가장 잘 알려진 두개의 선형 알고리즘
로지스틱 회귀(logistic), 서포트 백터 머신(SVC)
이 두모델은 기본적으로 L2규제를 적용(=Ridge) but, penalty = "l1"하면 L1규제 적용
규제 강도를 조절하는 것은 C
C가 높아지면 규제 감소,훈련세트에 가능한 최대에 맞추려하고(과대적합) , 낮으면 계수 백터(w)가 0에 가까워지도록 만든다.
회귀와 비슷하게 결정 경계가 직선이거나 평면이어서 제한적으로 보임
고차원 일때는 매우 강력해지고 특성이 많아지면 과대적합이 되지 않도록 해야한다.
*훈련세트와 테스트 세트가 비슷하면 과소적합

다중 클래스 분류용 선형 모델
대부분의 알고리즘은 이진 분류알고리즘이여서 다중 클래스 분류 알고리즘으로 확장하는 보편적 기법은 일대다 방법
각 클래스를 모든 클래스와 구분하도록 이진 모델을 학습 즉, 클래스의 수만큼 이진 분류 모델이 만들어짐
이중 가장 높은 점수를 내는 분류기의 클래스를 예측값으로 선택

선형 모델 매개변수 회귀 모델에선 alpha, 로지스틱(분류용)에선 C
alpha가 높을수록 C가 낮을수록 모델이 단순해짐
회귀모델에선 L1,L2규제를 뭐할지 정해야한다.(중요한 특성이 없다면 L2규제 사용)
선형 모델은 학습속도가 빠르고 예측도 빠르다.
큰 데이터 셋과  희소한 데이터 셋에서도 잘 작동한다.
이해가 쉽고 빠르다 하지만 계수의 값들이 왜 그런지 명확하지 않아 계수 분석하기 어려움
샘플에 비해 특성이 많을 때 잘 작동한다.

나이브 베이즈 분류기
선형모델과 매우 유사
로지스틱,SVC와 같은 분류기보다 훈련 속도는 빠른 편이지만 일반화 성능이 조금 뒤진다.
효과적인 이유는 각 특성을 개별로 취급해 파라미터를 학습하고 각 특성에서 클래스별 통계를 단순히 취합하기 때문이다.

결정 트리 (Decision tree)
결정 트리는 분류와 회귀문제에 널리 사용하는 모델(예/아니요 질문을 이어 나가면서 학습)
지도 학습 방식으로 데이터로부터 학습
계층적으로 영역을 분할해가는 알고리즘
아무조건없이 리프노드가 순수노드가 될 때 까지 진행하면 모델이 매우 복잡해지고 데이터에 과대적합됩니다.
과대적합을 막는 2가지 방법은 사전가지치기와 트리를 만든후 테이터 포인트가 적은 노드를 삭제 또는 병합하는 사후 가지치기로 나뉜다.
트리를 만드는 결정에 각 특성이 얼마나 중요한지를 평가하는 특성 중요도
특성 중요도의 합은 1이다.
중요도가 높은거부터 분류시작
결정트리가 나은점은 만들어진 모델을 쉽게 시각화할 수 있어서 비전문가도 이해하기 쉽다.
데이터 스케일에 구애받지 않는다.( 특성의 정규화나 표준화 같은 전처리 과정이 필요 없다.)
단점은 사전 가지치기를 해도 과대적합되는 경향이 있어 일반화 성능이 좋지 않다.

회귀결정 트리
분류 트리(결정트리)와 매우 비슷
훈련 데이터의 범위 밖의 포인트에 대해 예측할 수 없다.

결정트리의 앙상블
단일 결정 트리의 대안법이 앙상블
여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법
랜덤 포레스트와 그레이디언트 부스팅을 결정 트리에 사용

랜덤 포레스트
결정 트리의 최대 단점은 과대적합되는 경향이 있다는것
랜덤 포레스트는 이것을 회피할 수 있는 방법
기본적으로 조금씩 다른 여러 결정 트리의 묶음 -> 예측 성능이 유지되면서 과대 적이 줄어든다.
전략을 구현하려면 트리가 많아야하고 타깃 예측을 잘하며 다른트리와는 구별되어야 한다.
트리들이 달라질수있도록 트리 생성 시 무작위성 주입
랜덤하게 만드는 방법 2가지는 트리를 만들 때 사용하는 데이터 포인트를 무작위로 선택하는 방법
분할 테스트에서 특성을 무작위로 선택하는 방법이 있다.

부트스트랩 샘플을 생성 (이유는 완전한 독립적으로 만들어져야 하므로) -> 데이터 포인트 중에서 무작위로 데이터 횟수만큼 반복 추출
다른점은 각 노드에서 최선의 테스트를 찾는 것이 아니고 각 노드에서 후보 특성을 무작위로 선택 후 이 후보들 중에서 최선의 테스트를 찾는 것
몇개의 특성을 고를지는 max_features로 조정 가능
max_features 값을 크게 하면 랜덤 포레스트의 트리들은 매우 비슷해지고 가장 두드러진 특성을 이용해 데이터를 잘맞출것이다.
max_features값이 낮으면 랜덤 포레스트의 트리들이 달라지고 트리는 데이터에 맞추기위해 깊이가 깊어지게 된다.
랜덤 포레스트로 예측할 때는 모든 트리의 예측을 만든다.( 회귀의 경우 예측을 평균하여 최종 예측하게하고, 분류의 경우 약한 투표 전략)
랜덤 포레스트는 개개의 트리보다는 덜 과대적합되고 훨씬 좋은 결정 경계를 만들어준다.
트리가 많을수록 더 부드러운 결정 경계가 만들어진다.
랜덤 포레스트는 단일 트리보다 더 넓은 시각으로 데이터를 바라볼 수 있다.
회귀와 분류에 있어서 가장 널리 사용되는 머신러닝 알고리즘이다.
랜덤 포레스트는 성능이 매우 뛰어나고 매개 변수 튜닝을 많이 하지 않아도 잘 작동하며 데이터의 스케일을 맞출 필요가 없다.
선형모델이 적합하다.(매우 차원이 높고 희소한 데이터에는 잘 작동하지 않는다)
n_estimators(트리)가 많으면 많을수록 좋다.

* 커널 서포트 벡터 머신(SVM)
회귀 분류에서 사용 가능 하지만,  SVC를 사용하는 분류 문제만을 다룬다.
저차원 데이터셋에서는 선형 모델이 매우 제한적 but 유연하게 만드는 방법은 특성끼리 곱하거나
특성을 거듭제곱하는 식으로 특성을 추가하는 것이다.( 비 선형 특성을 추가한다)

1.커널 기법
특성이 많은 경우 어떤 특성을 추가해야 할지 모르고 특성을 많이 추가하면 연산비용이 커진다.
하지만 커널 기법은 고차원에서 분류기를 학습시킬  수 있다.(=커널기법)
실제 데이터를 확장하지 않고 확장된 특성에 대한 데이터 포인트들의 거리를 계산
고차원 공간에 매핑하는데 2가지 방법이 사용
특성의 가능한조합을 지정된 차수까지 모두 계산 or RBF(가우시안 커널)이 있다. (= 모든 차수의 모든 다항식을 고려한다)

SVM(support vater model) : 훈련 데이터의 일부만 결정 경꼐를 만드는데 영향을 준다. 두클래스 사이의 경계에 위치 한데이터 포인트들
유클리디안 거리와 r(감마)는 가우시안 커널의 폭을 제어하는 매개변수를 알아야한다.

2.RBF커널
gamma 매개변수는 r이고 가우시안 커널 폭이 역수에 해당
작으면 넓은 영역을 뜻하며 큰 값이라면 영향을 미치는 범위가 제한적이다.
가우시안 커널의 반경이 클수록 훈련 샘플의 영향 범위도 커진다.
각 포인트의 중요도(dual_coef_값)을 제한한다.
작은 gamma값은 모델 복잡도를 낮추고 큰값은 복잡한 모델을 만든다.
작은 C는 매우 제약이 큰모델을 만들고 데이터 포인터의 영향력도 작습니다.
C가 증가하면 포인터들이 모델에 큰영향을 주며 정확하게 분류한다.

#장단점 매개변수
SVM은 데이터 특성이 몇개 안되더라도 복잡한 결정 경계를 만들 수 있다.
샘플이 많지않으면 저차원 고차원 데이터에서 잘 맞지 않는다.
데이터전처리와 매개변수 설정에 신경을 많이 써야한다


요약 및 정리
최근접 이웃 : 작은 데이터셋일 경우, 기본모델로서 좋고 설명하기 쉬움

선형 모델 : 첫 번째로 시도할 아록리즘, 대용량 데이터 셋 가능, 고차원 데이터에 가능

결정 트리 : 매우빠름, 데디터 스케일 조정이 필요없음, 시각화하기 좋고 설명하기 쉬움

랜덤 포레스트 : 결정 트리 하나보다 더 좋은 성능을 냄, 매우 안정적이고 강력함, 데이터 스케일 조정 필요 없음, 고차원 희소 데이터에는 잘 안 맞음.

그레디언트 부스팅 결정 트리 : 랜덤 포레스트보다 성능이 좋음, 학습이 느리나 예측은 빠르다. 메모리를 조금 사용, 고차원 희소 데이터에는 잘 안 맞음

서포트 벡터 머신(SVM,SVC) :비슷한 의미의 특성으로 이뤄진 중간 규모의 데이터 셋에 잘 맞음, 데이터 스케일 조정 필요, 매개변수에 민감

비지도 학습

비지도 변환은 데이터를 새롭게 표현하여 원래 데이터보다쉽게 해석할 수있도록 만드는 알고리즘
특성 수가 많은 고차원 데이터를 특성의 수를 줄이면서 꼭 필요한 특징을 포함한 데이터로 표현하는 방법인 차원 축소가 있다.
2차원으로 변경하는 경우이다
군집 알고리즘은 데이터를 비슷한 것끼리 그룹으로 묶는것

데이터 전처리와 스케일 조정
standard scler = 특성의 평균을 0, 분산을 1로 변경하여 모든 특성이 같은 크기를 가지게 한다.(z-score)
특성의 최솟값과 크기를 제한하진 않는다.
minmaxscaler = 모든 특성을 정확하게 0과 1사이에 위치하게끔 한다.
2차원 데이터일 경우에는 x = 0 ,1 y 도 0,1 사이의 사각 영역에 담기게 된다.
normaizer = 매우 다른 스케일 조정
특성 벡터의 유클리디안 길이가 1이되도록 데이터 포인트를 조정 = 지름이 1인 원에 데이터 포인트를 투영합니다.
다른말로 각 데이터 포인트가 다른 비율로 스케일에 조정된다는 뜻

데이터를 변환하는 이유 : 시각화,데이터를 압축 , 추가적인 처리를 위해 정보가 더 잘 드러나는 표현을 찾기 위해서

주성분 분석(PCA)
특성들이 통계적으로 상관관계가 없도록 데이터셋을 회전시키는 기술(p 194 꼭 보기바람)
일반적으로 원본 특성 개수만큼의 주성분이 있다.
특성을 제거하여 2차원 데이터 셋이 1차원 데이터 셋으로 차원이 감소한다.
PCA를 적용하기 전에 standardscaler를 사용해 각 특서의 분산이 1이 되도록 데이터 스케일 조정이 필요
비지도 학습이기 때문에 어떤한 클래스 정보도 사용하지 않는다.
데이터의 상관관계만을 고려합니다.
단점은 그래프 두 축을 해석하기가 쉽지 않다는 것이다.
PCA객체가 학습될때 components_속성에 주성분이 저장됩니다

군집(clustering)
K-평균 군집 :  가장 간단하고 널리퍼진 알고리즘
클러스터 중심을 찾는다. 가장 가까운 클러스터 중심에 할당하고, 다음 클러스터에 할당된 데이터 포인트의 평균으로 클러스터 중심을 다시 지정합니다.
클러스터에 할당되는 데이터 포인트에 변화가 없을 때 종료됩니다.
K-평균 알고리즘이 실패하는 경우
중심이 하나뿐이므로 클러스터는 둥근 형태로 나난다.
K-평균은 가장 가까운 클러스터 중심까지의 거리만 고려하기 때문에 이런 데이터를 잘 처리하지 못합니다.
(모여있지않고 분산되어있는 값은 클러스터 사용하기가 껄끄럽다)

범주형 변수
원-핫-인코딩
가변수를 범주형 변수를 0 또는 1값을 가진 하나 이상의 새로운 특성으로 바꾼 것이다.
0과 1로 표현된 변수는 선형 이진 공식에 적용할 수 있다.
범주형 값이 같은 방식으로 표현되야 하기 때문에 dummy함수를 사용한다.
columnTransformer 클래스는 입력 데이터에 있는 열마다 다른 변환을 적용할 수 있다.
연속형 특성과 범주형 특성은 매우 다른 전 처리 과정이 필요하여 이클래스는 매우 유용

구간 분할
연속형 데이터에 아주 강력한 선형 모델을 만드는 방법 하나는 한 특성을 여러 특성으로 나누는 구간 분할입니다.
용량이 매우 크고 고차원 데이터 셋이라 선형 모델을 사용해야 한다면 구간 분할이 모델 성능을 높이는데 아주 좋은 방법

특성을 풍부하게 나타내는 또하나의 방법은 상호작용과 다항식 추가하는 것
이것은 특성을 확장하는 방법이다.
선형모델은 이런 절편 외에도 기울기도 학습할 수 있다.
추가하는 방법은 구간으로 분할된 데이터에 원래 특성을 다시 추가하는 것입니다.
11차원 데이터셋이 만들어진다. 

이렇게만 추가하면 각 구간 기울기가 같이 나와 효과가 없다.
그래서 구간 특성과 원본 특성을 곱한다.(기울기를 다르게 표현하기 위해)
이것이 상호작용 특성이다.
다항식을 추가하는 방법도 있다.
하지만 고차원 다항식은 데이터가 부족한 영역에서 너무 민감하게 동작합니다.
degree = 2 라면 특성의 제곱은 물론 가능한 두 특성의 조합을 모두 포함합니다.
상수항 1인 특성도 있다.

일변량 비선형 변환
log, exp, sin 같은 수학 함수를 적용하는 방법도 특성 변환에 유용하다.
트리기반 모델은 특성의 순서에만 영향을 받지만, 선형 모델과 신경망은 각 특성의 스케일과 분포에 밀접하게 연관되어 있습니다.
log,exp 함수는 데이터의 스케일을 변경해 선형 모데과 신경망의 성능을 올리는데 도움을 준다. ( sin, cos 함수는 주기적인 패턴이 들어있는 데이터를 다룰 때 편리합니다.)
정규분포와 비슷할 때 최고의 성능을 낸다.

특성 자동 선택
고차원 데이터 셋이면 가장 유용한 특성만 선택하고 나머지는 무시하는 것이 성능이 좋다.
어ㄸ?ㄴ 특성이 좋을지 아는 방법은 일변량 통계, 모델 기반 선택, 반복적 선택이 있다.
세 방법 모두 지도 방법이다.(훈련세트와 테스트 세트로 나눠야 한다는 말)

일변량 통계=분산 분석
즉, 각 특성이 독립적으로 평가된다는 점이다.
깊게 연관된 특성은 선택되지 않을 것이다.
일변량 분석은 계산이 매우 빠르고, 평가를 위해 모델을 만들 필요가 없다.
분류에서는 f_classif(기본값)을, 회귀에서는 f_regression을 선택하여 테스트하고, 계산한 p-값에 기초하여 특성을제외하는 방식을 선택한다.

모델 기반 선택
지도 학습 머신러닝 모델을 사용하여 특성의 중요도를 평가해서 가장 중요한 특성들만 선택합니다. 특성 선택을 위한 모델은 각 특성의 중요도를 측정하여 순서를 매길 줄 알아야한다.
결정 트리와 이를 기반으로한 모델을 사용한다. 선형모델 계수의 절댓값도 특성의 중요도를 제는데 사용할 수 있다. 일변량 분석과는 반대로 모델기반 특성 선택은 한번에 모든 특성을 고려하므로 상호작용 부분을 반영할 수 있다.
일변량 분석보다는 훨씬 강력하고 매우 복잡한 모델이다.
반복적 특성 선택
특성의 수가 각기 다른 일련의 모델이 만들어진다.
방법은 특성을 하나도 선택하지 않은 상태로 시작해서 어떤 종료 조건까지 하나씩 추가하는 방법과 모든 특성을 가지고 시작해서 어ㄸ?ㄴ 종료 조건이 될 때까지 특성을 하나씩 제거해 가는 
방법이다.
계산비용이 훨씬 많이든다. 재귀적 특성제거가 이런 방법의 하나이다.
재귀적 특성 제거는 모든 특성으로 시작해서 모델을 만들고 특성 중요도가 가장 낮은 특성을 제거한다. 제거한 특성을 빼고 나머지 특성 전체로 새로운 모델을 만듭니다.
미리 정의한 특성 개수가 남을 때까지 계속합니다.
특성 선택에 사용할 모델은 특성의 중요도를 결정하는 방법을 제공해야한다.
